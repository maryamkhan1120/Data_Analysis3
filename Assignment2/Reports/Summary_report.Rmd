---
title: "Summary Report - Assignment 2"
author: "Maryam Khan"
output: 
  pdf_document:
    extra_dependencies: ["flafter"]
---

```{r message=FALSE, warning=FALSE, include=FALSE}
#### SET UP
# It is advised to start a new session for every case study
# CLEAR MEMORY
rm(list=ls())

getwd()
setwd("/Users/maryamkhan/Data_Analysis3/Assignment2")
# Descriptive statistics and regressions
library(tidyverse)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(stargazer)
library(xtable)
library(directlabels)
library(knitr)
library(cowplot)
library(rattle)
library(ranger)
library(Hmisc)
library(kableExtra)
library(ggcorrplot)
library(ggplot2)
library(ggpubr)
library(pdp)
library(kableExtra)
library(rpart)
library(rpart.plot)

# set data dir, load theme and functions
path <- "/Users/maryamkhan/Data_Analysis3/Assignment2/"

source(paste0(path, "da_helper_functions.R"))
source(paste0(path, "theme_bg.R"))

# data used
data_in <- paste0(path, "data/Clean/")
data_out <- paste0(path, "data/Clean/")
output <- paste0(path,"Output/")


options(digits = 3)


```

```{r message=FALSE, warning=FALSE, include=FALSE}
#############
# Load data #
#############

data <-
  read_csv(paste0(data_in, "istanbul_workfile_adj.csv")) %>%
  mutate_if(is.character, factor)

```

```{r message=FALSE, warning=FALSE, include=FALSE}

######################
# Quick look at data #
######################
glimpse(data)


# where do we have missing variables now?
to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

#####################################
# Look at some descriptive statistics
#####################################

#How is the average price changing  by `property_type`,?
data %>%
  group_by(property_type) %>%
  summarise(count=n())
# dplyr::summarize(mean_price = mean(price, na.rm=TRUE)) %>%
# summarise(count=n())

# boxplot of price by property type
data$f_property_type


price_vs_property_box <- ggplot(data = data, aes(x = property_type, y = price)) +
  stat_boxplot(aes(group = property_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1],color[3]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = property_type),
               color = c(color[2],color[1],color[3]), fill = c(color[2],color[1],color[3]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  #scale_y_continuous(expand = c(0.01,0.01),limits = c(0,65), breaks = seq(0,30,65)) +
  labs(x = "Property type",y = "Price (USA)")+
  theme_classic()
price_vs_property_box


# Barchart  
fig4 <- ggplot(data = data, aes(x = factor(n_accommodates), color = f_property_type, fill = f_property_type)) +
  geom_bar(alpha=0.6, na.rm=T, width = 0.8) +
  scale_color_manual(name="",
                     values=c(color[2],color[1],color[3])) +
  scale_fill_manual(name="",
                    values=c(color[2],color[1],color[3])) +
  labs(x = "Accomodates (Persons)",y = "Frequency")+
  theme_classic()
fig4


```

```{r message=FALSE, warning=FALSE, include=FALSE}
#######################################
# PART II.
########################################




#####################
# Setting up models #
#####################

# Basic Variables
basic_lev  <- c("f_property_type", "n_accommodates", "n_beds",  "n_days_since_last", "flag_days_since_last")


# Factorized variables
basic_add <- c("f_bathroom", "f_bedrooms","flag_n_bedrooms", "f_neighbourhood_cleansed", "f_minimum_nights", "n_availability_365")


reviews <- c("n_review_scores_rating", "flag_review_scores_rating","f_review_scores_rating",
             "n_number_of_reviews","f_number_of_reviews","n_reviews_per_month","flag_reviews_per_month")

host <- c("d_host_is_superhost", "d_host_identity_verified")



# Dummy variables: Extras -> collect all options and create dummies
dummies <-  grep("^d_.*", names(data), value = TRUE)        



```

```{r message=FALSE, warning=FALSE, include=FALSE}
#################################
# Look for interactions         #
#################################

## This huge correlation table shows how strongly numeric variables are correlated
num_data <- data[,unlist(lapply(data, is.numeric))]  
num_data <- num_data %>%  select(matches("^d_.*|^n_.*|^f_.*|^p.*"))

corr <- round(cor(num_data), 1)
ggcorrplot(corr)



# Plot interactions between room type/property type and all dummies
sapply(dummies, function(x){
  p <- price_diff_by_variables2(data, "f_property_type", x, "property_type", x)
  print(p)
})

data %>% 
  group_by(f_property_type) %>% 
  summarise(cnt = n())


interactions <- c("f_property_type*d_baking_sheet",
                  "f_property_type*d_barbecue_utensils",
                  "f_property_type*d_bathtub",
                  "f_property_type*d_beachfront",
                  "f_property_type*d_board_games",
                  "f_property_type*d_carbon_monoxide_alarm",
                  "f_property_type*d_cleaning_before_checkout",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_elevator",
                  "f_property_type*d_essentials",
                  "f_property_type*d_ev_charger",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_first_aid_kit",
                  "f_property_type*d_freezer",
                  "f_property_type*d_hangers",
                  "f_property_type*d_hot_tub",
                  "f_property_type*d_hot_water_kettle",
                  "f_property_type*d_keypad",
                  "f_property_type*d_lake_access",
                  "f_property_type*d_lockbox",
                  "f_property_type*d_long_term_stays_allowed",
                  "f_property_type*d_luggage_dropoff_allowed",
                  "f_property_type*d_microwave",
                  "f_property_type*d_mini_fridge",
                  "f_property_type*d_mosquito_net",
                  "f_property_type*d_outdoor_dining_area",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_outlet_covers",
                  "f_property_type*d_pool",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_private_hot_tub",
                  "f_property_type*d_safe",
                  "f_property_type*d_shared_pool",
                  "f_property_type*d_single_level_home",
                  "f_property_type*d_smoke_alarm",
                  "f_property_type*d_toaster",
                  "f_property_type*d_trash_compactor",
                  "f_property_type*d_window_ac_unit",
                  "f_property_type*d_window_guards",
                  "f_property_type*d_wine_glasses",
                  "f_property_type*d_have_kitchen",
                  "f_property_type*d_have_stove",
                  "f_property_type*d_have_oven",
                  "f_property_type*d_fridge",
                  "f_property_type*d_coffee_machine",
                  "f_property_type*d_have_gril",
                  "f_property_type*d_free_parking_on_premises",
                  "f_property_type*d_paid_parking_off_premises",
                  "f_property_type*d_wifi",
                  "f_property_type*d_have_tv",
                  "f_property_type*d_have_sound_system",
                  "f_property_type*d_shampoo_conditioner",
                  "f_property_type*d_have_washer",
                  "f_property_type*d_have_dryer",
                  "f_property_type*d_have_iron",
                  "f_property_type*d_have_air_condfan",
                  "f_property_type*d_balcony",
                  "f_property_type*d_have_breakfast",
                  "f_property_type*d_have_workoffice",
                  "f_property_type*d_have_fitnessgym",
                  "f_property_type*d_family_friendly",
                  "f_property_type*d_have_fireplace",
                  "f_property_type*d_have_clothing_storage",
                  "f_property_type*d_host_is_superhost",
                  "f_property_type*d_host_identity_verified")


```

```{r message=FALSE, warning=FALSE, include=FALSE}
#################################
# Create test and train samples #
#################################
# now all stuff runs on training vs test (holdout), alternative: 5-fold CV


# create test and train samples (80% of observations in train sample)
smp_size <- floor(0.8 * nrow(data))

## K = 5
k_folds <- 5
# Define seed value
seed_val <- 95

train_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$train <- 0
data$train[train_ids] <- 1
# Create train and test sample variables
data_train <- data %>% filter(train == 1)
data_test <- data %>% filter(train == 0)

#Building the most complex model to use in LASSO
model4 <- paste0(" ~ ",paste(c(basic_lev, basic_add ,host,reviews, dummies, interactions),collapse = " + "))


# Creating the most complex OLS model to run a LASSO. Here LASSO is being used as a tool to choose predictors

# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
formula <- formula(paste0("price ", paste(setdiff(model4, "price"), collapse = " + ")))

# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_train,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)
# Check the output
lasso_model
# Penalty parameters
lasso_model$bestTune
# Check th optimal lambda parameter
lasso_model$bestTune$lambda
# Check the RMSE curve
plot(lasso_model)

# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed

lasso_coeffs
print(lasso_coeffs)


# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))

lasso_coeffs_nz

#write_csv(lasso_coeffs_nz,"NonZeroCoefficients.csv")

# Get the RMSE of the Lasso model
#   Note you should compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda)
lasso_fitstats

# Create an auxilary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA,
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )


# modifying the list of variables to be used based on LASSO results

dummies <- c("d_backyard",
             "d_baking_sheet",
             "d_beachfront",
             "d_bidet",
             "d_bikes",
             "d_bread_maker",
             "d_carbon_monoxide_alarm",
             "d_drying_rack_for_clothing",
             "d_essentials",
             "d_extra_pillows_and_blankets",
             "d_fire_extinguisher",
             "d_hot_tub",
             "d_hot_water",
             "d_hot_water_kettle",
             "d_lockbox",
             "d_long_term_stays_allowed",
             "d_microwave",
             "d_mini_fridge",
             "d_mosquito_net",
             "d_outdoor_dining_area",
             "d_outdoor_furniture",
             "d_private_hot_tub",
             "d_smoke_alarm",
             "d_trash_compactor",
             "d_waterfront",
             "d_window_ac_unit",
             "d_window_guards",
             "d_have_kitchen",
             "d_have_oven",
             "d_coffee_machine",
             "d_free_parking_on_street",
             "d_paid_parking_off_premises",
             "d_wifi",
             "d_have_sound_system",
             "d_have_washer",
             "d_have_dryer",
             "d_have_air_condfan",
             "d_balcony",
             "d_have_garden",
             "d_have_fitnessgym",
             "d_family_friendly",
             "d_have_fireplace")


interactions <- c("f_property_type*d_baking_sheet",
                  "f_property_type*d_barbecue_utensils",
                  "f_property_type*d_beachfront",
                  "f_property_type*d_beachfront",
                  "f_property_type*d_carbon_monoxide_alarm",
                  "f_property_type*d_cleaning_before_checkout",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_elevator",
                  "f_property_type*d_elevator",
                  "f_property_type*d_ev_charger",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_hot_tub",
                  "f_property_type*d_keypad",
                  "f_property_type*d_lake_access",
                  "f_property_type*d_long_term_stays_allowed",
                  "f_property_type*d_luggage_dropoff_allowed",
                  "f_property_type*d_microwave",
                  "f_property_type*d_mosquito_net",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_pool",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_private_hot_tub",
                  "f_property_type*d_safe",
                  "f_property_type*d_single_level_home",
                  "f_property_type*d_toaster",
                  "f_property_type*d_trash_compactor",
                  "f_property_type*d_window_ac_unit",
                  "f_property_type*d_have_gril",
                  "f_property_type*d_have_tv",
                  "f_property_type*d_have_dryer",
                  "f_property_type*d_balcony",
                  "f_property_type*d_have_breakfast",
                  "f_property_type*d_have_fitnessgym",
                  "f_property_type*d_family_friendly",
                  "f_property_type*d_have_fireplace",
                  "f_property_type*d_have_clothing_storage",
                  "f_property_type*d_host_identity_verified")

basic_lev  <- c("f_property_type",  "n_accommodates", "n_beds",  "n_days_since_last", "flag_days_since_last")

# Factorized variables
basic_add <- c("f_neighbourhood_cleansed","f_bathroom", "f_bedrooms","f_minimum_nights", "n_availability_365")


reviews <- c("n_reviews_per_month","flag_reviews_per_month")

host <- c("d_host_is_superhost", "d_host_identity_verified")



# Building OLS models

model1 <- " ~ n_accommodates"
model2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
model3 <- paste0(" ~ ",paste(c(basic_lev, basic_add,reviews,host, dummies ),collapse = " + "))



```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Do the iteration

library(fixest)
for ( i in 1:4 ){
  print(paste0( "Estimating model: " ,i ))
  # Get the model name
  model_name <-  paste0("model",i)
  model_pretty_name <- paste0("M",i,"")
  # Specify the formula
  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))
  
  # Estimate model on the whole sample
  model_work_data <- feols( formula , data = data_train , vcov='hetero' )
  #  and get the summary statistics
  fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
  BIC <- fs$bic
  r2  <- fs$r2
  rmse_train <- fs$rmse
  ncoeff <- length( model_work_data$coefficients )
  
  # Do the k-fold estimation
  set.seed(seed_val)
  cv_i <- train( formula, data_train, method = "lm",
                 trControl = trainControl(method = "cv", number = k_folds))
  rmse_test <- mean( cv_i$resample$RMSE )
  
  # Save the results
  model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                      R_squared=r2, BIC = BIC,
                      Training_RMSE = rmse_train, Test_RMSE = rmse_test )
  if ( i == 1 ){
    model_results <- model_add
  } else{
    model_results <- rbind( model_results , model_add )
  }
}



# Check summary table
# Add it to final results

model_results <- rbind( model_results , lasso_add )
model_results

## As per these results, model4 is clearly over fitted as the R-squared comes out to be 1 with a negative BIC.##
## The purpose of model4 was primarily to include all the relevant variables and use it in LASSO to identify predictors with non-zero coefficients.##



```


```{r message=FALSE, warning=FALSE, include=FALSE}
#before random forest

predictors_model3 <- c(basic_lev, basic_add, dummies, host , reviews)
set.seed(95)
system.time({
  ols_model <- train(
    formula(paste0("price ~", paste0(predictors_model3, collapse = " + "))),
    data = data_train,
    method = "lm",
    trControl = train_control
  )
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

ols_model_coeffs_df



# Random Forest

predictors <- c(basic_lev,basic_add,host, reviews, dummies,interactions)

# set tuning
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)
set.seed(95)
system.time({
  rf_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})

rf_model

# auto tuning first - gives 92 predictors

set.seed(95)
system.time({
  rf_model_auto <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    importance = "impurity"
  )
})
rf_model_auto

rf_model_table <- rf_model$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)

##Variable Importance Plots rf_model

rf_model_var_imp <- ranger::importance(rf_model$finalModel)/1000

rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_var_imp_df

# to have a quick look
plot(varImp(rf_model))


# have a version with top 10 vars only

var_imp10 <- ggplot(rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```

```{r message=FALSE, warning=FALSE, include=FALSE}
##############################
# 2) varimp plot grouped
##############################

# grouped variable importance - keep binaries created off factors together
varnames <- rf_model$finalModel$xNames

f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)

f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)

f_reviews_varnames <- grep("review",varnames, value = TRUE)


#dummies_varnames?
t <-  grep("d_",varnames, value = TRUE)
dummies_varnames <-  t[43:95]


groups <- list(
               Property_type = f_property_type_varnames,
               Reviews = f_reviews_varnames,
               Neighbourhood= f_neighbourhood_cleansed_varnames,
               Amenities = dummies_varnames,bathroom = "f_bathroom",
               last_review = "n_days_since_last",
               n_accommodates = "n_accommodates",
               availability_365="n_availability_365",
               n_beds = "n_beds"
               )

# Need a function to calculate grouped var-imp

group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                          imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

var_imp_group <- ggplot(rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

##Variable Importance Plots rf_model_auto

rf_model_auto_var_imp <- ranger::importance(rf_model_auto$finalModel)/1000
rf_model_auto_var_imp_df <-
  data.frame(varname = names(rf_model_auto_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood:", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
rf_model_auto_var_imp_df

# to have a quick look

plot(varImp(rf_model_auto))

# have a version with top 10 vars only

ggplot(rf_model_auto_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()
data$d_ <- NULL
colnames(data)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
##############################
# 2) varimp plot grouped

##############################
# grouped variable importance - keep binaries created off factors together

varnames_auto <- rf_model_auto$finalModel$xNames

f_neighbourhood_cleansed_varnames_auto <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_property_type_varnames_auto <- grep("f_property_type",varnames_auto, value = TRUE)
f_reviews_varnames_auto <- grep("review",varnames_auto, value = TRUE)
dummies_varnames_auto <- t[14:105]


groups_auto <- list(
                    property_type = f_property_type_varnames_auto,
                    reviews = f_reviews_varnames_auto,
                    neighbourhood=f_neighbourhood_cleansed_varnames_auto,
                    Ammenities = dummies_varnames_auto,
                    bathroom = "f_bathroom",
                    last_review = "n_days_sincelast",
                    n_accommodates = "n_accommodates",
                    availability_365="n_availability_365",
                    n_beds = "n_beds" )


# Need a function to calculate grouped var-imp

group.importance <- function(rf.obj, groups_auto) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_auto_var_imp_grouped <- group.importance(rf_model_auto$finalModel, groups)
rf_model_auto_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_auto_var_imp_grouped),
                                               imp = rf_model_auto_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

ggplot(rf_model_auto_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()



# evaluate random forests

results <- resamples(
  list(
    model_1  = rf_model,
    model_auto  = rf_model_auto
  )
)
summary(results)





# CART with built-in pruning

set.seed(95)
system.time({
  cart_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})

cart_model



# Tree graph

rpart.plot(cart_model$finalModel, tweak=1.2, digits=-1, extra=1)


```

```{r message=FALSE, warning=FALSE, include=FALSE}

# GBM

gbm_grid <-  expand.grid(interaction.depth = 5, # complexity of the tree
                         n.trees = 250, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)

set.seed(111)
system.time({
  gbm_model <- train(formula(paste0("price ~", paste0(predictors, collapse = " + "))),
                     data = data_train,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model
gbm_model$finalModel
# save( gbm_model , file = 'gbm_model.RData' )


# get prediction rmse and add to next summary table
# ---- compare these models

final_models <-
  list("OLS" = ols_model,
       "CART" = cart_model,
       "Random forest 1: Tuning provided" = rf_model,
       "Random forest 2: Auto Tuning" = rf_model_auto,
       "GBM"  = gbm_model)
results <- resamples(final_models) %>% summary()
results

# Model selection is carried out on this CV RMSE
result <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")
result




#########################################################################################
# Partial Dependence Plots for the best model; random forest with specified tuning parameters
#########################################################################################
# 1) Property Type
pdp_f_property_type <- pdp::partial(rf_model_auto, pred.var = "f_property_type",
                                    pred.grid = distinct_(data_test, "f_property_type"),
                                    train = data_train)
pdp_pptype <- pdp_f_property_type %>%
  autoplot( ) +
  geom_point(color='red', size=2) +
  geom_line(color='red', size=1) +
  ylab("Predicted price") +
  xlab("Property Type") +
  theme_bw()

# 2) Number of accommodates
pdp_n_accommodates <- pdp::partial(rf_model_auto, pred.var = "n_accommodates",
                                   pred.grid = distinct_(data_test, "n_accommodates"),
                                   train = data_train)
pdp_acctype <- pdp_n_accommodates %>%
  autoplot( ) +
  geom_point(color='red', size=4) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  #scale_y_continuous(limits=c(80,120), breaks=seq(80,120, by=10)) +
  theme_bw()

# 3) neighborhood

pdp_f_neighbourhood_cleansed <- pdp::partial(rf_model_auto, pred.var = "f_neighbourhood_cleansed",
                                                   pred.grid = distinct_(data_test, "f_neighbourhood_cleansed"),
                                                   train = data_train)


```


## Introduction

The purpose of this project is to help a small to mid-sized firm price an apartment that can accommodate 2 – 6 people in **Istanbul**. To help the firm make this decision I built a price prediction model using various regression techniques and models. The data for this prediction analysis was scraped from the [Inside Airbnb](http://insideairbnb.com/get-the-data.html) from 29 December 2021 to 31 December 2021. To make this prediction I will be looking at a few key predictors to make the prediction more accurate. These predictors some basic attributes about the property, reviews and ratings, a few variables regarding the host and various kinds of amenities that describe the apartment. For this project I will be using 5 different types of models which include the OLS, Cart, two different Random Forest and GBM. I will be using these models to pick my final prediction based on the prediction power looking at the RMSE and R squared values. 


## Data Cleaning and Preparation
To start working on the models, I had to start with some preliminary cleaning of the data. Before I began cleaning the data my raw data set contained 22,695 observations and 74 variables. I dropped some of the variables in the beginning that included urls and some descriptions that could not be used in the analysis. After doing some rudimentary cleaning I proceeded to sort out the amenities. For this I first split the amenities in different dummy variables and then they similar amenities were grouped together into one variable for example, the different WIFI and internet variables were all grouped together as one. This helped me in reducing the number of variables, initially, I had 2093 amenities which were reduced to 172 after being grouped together. For more details the data cleaning code has been uploaded here [data_cleaning.R](https://github.com/maryamkhan1120/Data_Analysis3/blob/main/Assignment2/codes/data_cleaning.R).
The main variable of interest, the price, was also converted to USD as in the raw data the prices were given in Turkish lira. Furthermore, the all the observations with missing price values were dropped. 
The data was further prepared for the analysis by filtering for apartments, condominium and lofts that can host 2 to 6 people as that is the requirement of our company. I also imputed some of the variables that contained missing values. Variables like number of bedrooms, bathrooms and the number of beds had missing values. After imputation of these predictors I had a data set which had no missing values. For more details the data preparation code has been uploaded here [data_prep.R](https://github.com/maryamkhan1120/Data_Analysis3/blob/main/Assignment2/codes/data_prep.R).

```{r echo=FALSE, fig.height=4, message=FALSE, warning=FALSE, out.width="50%"}
price_vs_property_box
fig4
```


## Variables
Another enormous challenge for this project was the future engineering and deciding which variables to include in my modelling. I did this by dividing my variables into groups of 5:

-	Numeric variables: These variables define size for example, number of beds, number of accommodates, number of bathrooms and bedrooms, the minimum number of nights required to rent the place.
-	Factor variables: These are categorical variables that are either in string or numeric form. For example, the neighborhood the apartment is in or the type of property (apartment, condominium, or a loft).
-	Dummy variables: These are binary variables mostly describing the amenities of a property like the WIFI, pool, beachfront facing property, kitchen supplies, elevator etc.
-	Review variables: These describe the reviews and ratings characteristics for an individual property. Number of reviews, the rating for those reviews and the mean monthly reviews received. 
-	Host variables: These variables describe the characteristics of the host of the property. They are binary variables like host is a superhost or if the host is verified. 

## Exploratory Data Analysis
Once all the data was cleaned and the variables were grouped, we looked at the main target variable which is the price. The price distribution was close to normal, hence, there was no need of taking a log of the variable. 
The next step was to look for interactions, for this I checked the relationship between property type and some of the amenities and if there was a noticeable price difference then the dummy variable was included as an interaction term.

## Modeling 
Once all the data was prepped for doing our regression analysis, I divided the data into the test and train samples, where 80% of the observations were part of the train sample and 20% of the observations were part of the test sample. These test and train samples were used in all machine learning models. The detailed code can be viewed here [data_prediction.R](https://github.com/maryamkhan1120/Data_Analysis3/blob/main/Assignment2/codes/data_prediction.R).

**Linear Regression:** I first started with building my OLS model and based on LASSO I selected all the non-zero coefficients and included them in my OLS models. The first model was only with n_accommodates variable, model 2 and 3 had 7 and 91 variables respectively and model 4 was the over-fitted model with all the variables. According to my OLS regression results, model 3 proved to be the best predictive model out of these 4.
Even though the BIC value increases after the model 1, the increase in the value is not that significant from model 1 to model 3 and model 3 has the lowest RMSE in the training set. Furthermore, looking at the R squared value of the model we can say that this model explains 34.7% of the variation in price. When BIC and Cross validation produce conflicting results, we should prefer to side with the cross-validation results since it’s not based on auxiliary assumptions (BEKES, 2021). Since the difference between them is small it will not be big mistake if I pick model 3 over model 1 and 2. 

**Random Forest:** The Random Forest software creates hundreds of regression trees and combines them in one and gives an average. Since it uses bootstrap aggregation, the model is able to produce accurate results. For this regression model I will be using the same training and holdout samples. Based on my regression results the lowest RMSE value was at 5 for the terminal nodes and 12  variables in each node giving me an RMSE value of 12.4.
I also used the auto tuned the same version of the model, in which the algorithm picks the variable and node values automatically. The cross-validated RMSE for autotune produced a cross-validated RMSE of 12.3 with 5 terminal nodes and 92 variables for each split. 

```{r echo=FALSE, message=FALSE, warning=FALSE,show.fig= "hold"}
result %>% kbl(caption = "Horse Race of Models CV RSME", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")
```


**Variable Importance:** The variable importance helps us in identifying the important variables that impact price. In my analysis the room availability, number of accommodates, days since last review, reviews per month and if the property has an air conditioner or fan were the top 5. The top 10 variables according to importance can be seen below. Furthermore, I grouped the variables to get a bird eye view as seen in the graph below and amenities account for 50% of the variable importance. 

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width = "50%", fig.height=4}
var_imp10
var_imp_group
 
```

**Partial Dependencies:** To further investigate I created partial dependency plot for the number of accommodates and the predicted price and as we can see in the graph below there is a fairly linear relationship between the number of people accommodated and price. We also created a plot with property type and price and as in the graph below it can be seen that renting a condo is more expensive than an apartment.


```{r echo=FALSE, message=FALSE, warning=FALSE,, out.width = "50%", fig.height=4}
pdp_pptype 
pdp_acctype

```

## Conclusion
After conducting this analysis, according to the results my best model was the Random Forest model with autotuning as that gave the lowest RMSE value. However, since Random Forest is more like a black-box model it is difficult to explain the details of the regressions to clients, therefore, I will suggest that they pick the OLS model. The difference in RSME values of both models is 40 cents which is not that big of a difference to avoid complexity. Therefore, the client should invest in either condo or apartments as they yield higher prices and since we are able to predict the prices better for 5 beds they should invest in a property with 5 beds.





