---
title: "Technical Report"
author: "Maryam Khan"
date: "2/10/2022"
output:
  html_document:
    code_download: true
---

```{r message=FALSE, warning=FALSE, include=FALSE}
#### SET UP
# It is advised to start a new session for every case study
# CLEAR MEMORY
rm(list=ls())

#getwd()
#setwd("/Users/maryamkhan/Data_Analysis3/Assignment2")
# Descriptive statistics and regressions
library(tidyverse)
library(caret)
library(skimr)
library(grid)
library(glmnet)
library(stargazer)
library(xtable)
library(directlabels)
library(knitr)
library(cowplot)
library(rattle)
library(ranger)
library(Hmisc)
library(kableExtra)
library(ggcorrplot)
library(ggplot2)
library(ggpubr)
library(pdp)
library(kableExtra)
library(rpart)
library(rpart.plot)

# set data dir, load theme and functions
path <- "/Users/maryamkhan/Data_Analysis3/Assignment2/"

source(paste0(path, "da_helper_functions.R"))
source(paste0(path, "theme_bg.R"))

# data used
data_in <- paste0(path, "data/Clean/")
data_out <- paste0(path, "data/Clean/")
output <- paste0(path,"Output/")


options(digits = 3)


```


```{r message=FALSE, warning=FALSE, include=FALSE}
# price graph
df <- read_rds(paste(data_in,"airbnb_istanbul_workfile.rds", sep = ""))


```


```{r message=FALSE, warning=FALSE, include=FALSE}
#############
# Load data #
#############

data <-
  read_csv(paste0(data_in, "istanbul_workfile_adj.csv")) %>%
  mutate_if(is.character, factor)

```

```{r message=FALSE, warning=FALSE, include=FALSE}

######################
# Quick look at data #
######################
glimpse(data)


# where do we have missing variables now?
to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

#####################################
# Look at some descriptive statistics
#####################################

#How is the average price changing  by `property_type`,?
data %>%
  group_by(property_type) %>%
  summarise(count=n())
# dplyr::summarize(mean_price = mean(price, na.rm=TRUE)) %>%
# summarise(count=n())

# boxplot of price by property type
data$f_property_type


price_vs_property_box <- ggplot(data = data, aes(x = property_type, y = price)) +
  stat_boxplot(aes(group = property_type), geom = "errorbar", width = 0.3,
               color = c(color[2],color[1],color[3]), size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = property_type),
               color = c(color[2],color[1],color[3]), fill = c(color[2],color[1],color[3]),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  #scale_y_continuous(expand = c(0.01,0.01),limits = c(0,65), breaks = seq(0,30,65)) +
  labs(x = "Property type",y = "Price (USA)")+
  theme_classic()
price_vs_property_box


# Barchart  
fig4 <- ggplot(data = data, aes(x = factor(n_accommodates), color = f_property_type, fill = f_property_type)) +
  geom_bar(alpha=0.6, na.rm=T, width = 0.8) +
  scale_color_manual(name="",
                     values=c(color[2],color[1],color[3])) +
  scale_fill_manual(name="",
                    values=c(color[2],color[1],color[3])) +
  labs(x = "Accomodates (Persons)",y = "Frequency")+
  theme_classic()
fig4


```

```{r message=FALSE, warning=FALSE, include=FALSE}
#######################################
# PART II.
########################################




#####################
# Setting up models #
#####################

# Basic Variables
basic_lev  <- c("f_property_type", "n_accommodates", "n_beds",  "n_days_since_last", "flag_days_since_last")


# Factorized variables
basic_add <- c("f_bathroom", "f_bedrooms","flag_n_bedrooms", "f_neighbourhood_cleansed", "f_minimum_nights", "n_availability_365")


reviews <- c("n_review_scores_rating", "flag_review_scores_rating","f_review_scores_rating",
             "n_number_of_reviews","f_number_of_reviews","n_reviews_per_month","flag_reviews_per_month")

host <- c("d_host_is_superhost", "d_host_identity_verified")



# Dummy variables: Extras -> collect all options and create dummies
dummies <-  grep("^d_.*", names(data), value = TRUE)        



```

```{r message=FALSE, warning=FALSE, include=FALSE}
#################################
# Look for interactions         #
#################################

## This huge correlation table shows how strongly numeric variables are correlated
num_data <- data[,unlist(lapply(data, is.numeric))]  
num_data <- num_data %>%  select(matches("^d_.*|^n_.*|^f_.*|^p.*"))

corr <- round(cor(num_data), 1)
ggcorrplot(corr)



# Plot interactions between room type/property type and all dummies
sapply(dummies, function(x){
  p <- price_diff_by_variables2(data, "f_property_type", x, "property_type", x)
  print(p)
})

data %>% 
  group_by(f_property_type) %>% 
  summarise(cnt = n())


interactions <- c("f_property_type*d_baking_sheet",
                  "f_property_type*d_barbecue_utensils",
                  "f_property_type*d_bathtub",
                  "f_property_type*d_beachfront",
                  "f_property_type*d_board_games",
                  "f_property_type*d_carbon_monoxide_alarm",
                  "f_property_type*d_cleaning_before_checkout",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_elevator",
                  "f_property_type*d_essentials",
                  "f_property_type*d_ev_charger",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_first_aid_kit",
                  "f_property_type*d_freezer",
                  "f_property_type*d_hangers",
                  "f_property_type*d_hot_tub",
                  "f_property_type*d_hot_water_kettle",
                  "f_property_type*d_keypad",
                  "f_property_type*d_lake_access",
                  "f_property_type*d_lockbox",
                  "f_property_type*d_long_term_stays_allowed",
                  "f_property_type*d_luggage_dropoff_allowed",
                  "f_property_type*d_microwave",
                  "f_property_type*d_mini_fridge",
                  "f_property_type*d_mosquito_net",
                  "f_property_type*d_outdoor_dining_area",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_outlet_covers",
                  "f_property_type*d_pool",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_private_hot_tub",
                  "f_property_type*d_safe",
                  "f_property_type*d_shared_pool",
                  "f_property_type*d_single_level_home",
                  "f_property_type*d_smoke_alarm",
                  "f_property_type*d_toaster",
                  "f_property_type*d_trash_compactor",
                  "f_property_type*d_window_ac_unit",
                  "f_property_type*d_window_guards",
                  "f_property_type*d_wine_glasses",
                  "f_property_type*d_have_kitchen",
                  "f_property_type*d_have_stove",
                  "f_property_type*d_have_oven",
                  "f_property_type*d_fridge",
                  "f_property_type*d_coffee_machine",
                  "f_property_type*d_have_gril",
                  "f_property_type*d_free_parking_on_premises",
                  "f_property_type*d_paid_parking_off_premises",
                  "f_property_type*d_wifi",
                  "f_property_type*d_have_tv",
                  "f_property_type*d_have_sound_system",
                  "f_property_type*d_shampoo_conditioner",
                  "f_property_type*d_have_washer",
                  "f_property_type*d_have_dryer",
                  "f_property_type*d_have_iron",
                  "f_property_type*d_have_air_condfan",
                  "f_property_type*d_balcony",
                  "f_property_type*d_have_breakfast",
                  "f_property_type*d_have_workoffice",
                  "f_property_type*d_have_fitnessgym",
                  "f_property_type*d_family_friendly",
                  "f_property_type*d_have_fireplace",
                  "f_property_type*d_have_clothing_storage",
                  "f_property_type*d_host_is_superhost",
                  "f_property_type*d_host_identity_verified")


```

```{r message=FALSE, warning=FALSE, include=FALSE}
#################################
# Create test and train samples #
#################################
# now all stuff runs on training vs test (holdout), alternative: 5-fold CV


# create test and train samples (80% of observations in train sample)
smp_size <- floor(0.8 * nrow(data))

## K = 5
k_folds <- 5
# Define seed value
seed_val <- 95

train_ids <- sample(seq_len(nrow(data)), size = smp_size)
data$train <- 0
data$train[train_ids] <- 1
# Create train and test sample variables
data_train <- data %>% filter(train == 1)
data_test <- data %>% filter(train == 0)

#Building the most complex model to use in LASSO
model4 <- paste0(" ~ ",paste(c(basic_lev, basic_add ,host,reviews, dummies, interactions),collapse = " + "))


# Creating the most complex OLS model to run a LASSO. Here LASSO is being used as a tool to choose predictors

# Set lasso tuning parameters:
# a) basic setup
train_control <- trainControl( method = "cv", number = k_folds)
# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))
# c) create a formula
formula <- formula(paste0("price ", paste(setdiff(model4, "price"), collapse = " + ")))

# Run LASSO
set.seed(seed_val)
lasso_model <- caret::train(formula,
                            data = data_train,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = train_control,
                            tuneGrid = tune_grid,
                            na.action=na.exclude)
# Check the output
lasso_model
# Penalty parameters
lasso_model$bestTune
# Check th optimal lambda parameter
lasso_model$bestTune$lambda
# Check the RMSE curve
plot(lasso_model)

# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed

lasso_coeffs
print(lasso_coeffs)


# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))

lasso_coeffs_nz

#write_csv(lasso_coeffs_nz,"NonZeroCoefficients.csv")

# Get the RMSE of the Lasso model
#   Note you should compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda)
lasso_fitstats

# Create an auxilary tibble
lasso_add <- tibble(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    R_squared=lasso_fitstats$Rsquared, BIC = NA,
                    Training_RMSE = NA, Test_RMSE = lasso_fitstats$RMSE )


# modifying the list of variables to be used based on LASSO results

dummies <- c("d_backyard",
             "d_baking_sheet",
             "d_beachfront",
             "d_bidet",
             "d_bikes",
             "d_bread_maker",
             "d_carbon_monoxide_alarm",
             "d_drying_rack_for_clothing",
             "d_essentials",
             "d_extra_pillows_and_blankets",
             "d_fire_extinguisher",
             "d_hot_tub",
             "d_hot_water",
             "d_hot_water_kettle",
             "d_lockbox",
             "d_long_term_stays_allowed",
             "d_microwave",
             "d_mini_fridge",
             "d_mosquito_net",
             "d_outdoor_dining_area",
             "d_outdoor_furniture",
             "d_private_hot_tub",
             "d_smoke_alarm",
             "d_trash_compactor",
             "d_waterfront",
             "d_window_ac_unit",
             "d_window_guards",
             "d_have_kitchen",
             "d_have_oven",
             "d_coffee_machine",
             "d_free_parking_on_street",
             "d_paid_parking_off_premises",
             "d_wifi",
             "d_have_sound_system",
             "d_have_washer",
             "d_have_dryer",
             "d_have_air_condfan",
             "d_balcony",
             "d_have_garden",
             "d_have_fitnessgym",
             "d_family_friendly",
             "d_have_fireplace")


interactions <- c("f_property_type*d_baking_sheet",
                  "f_property_type*d_barbecue_utensils",
                  "f_property_type*d_beachfront",
                  "f_property_type*d_beachfront",
                  "f_property_type*d_carbon_monoxide_alarm",
                  "f_property_type*d_cleaning_before_checkout",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_dining_table",
                  "f_property_type*d_elevator",
                  "f_property_type*d_elevator",
                  "f_property_type*d_ev_charger",
                  "f_property_type*d_fire_extinguisher",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_fire_pit",
                  "f_property_type*d_hot_tub",
                  "f_property_type*d_keypad",
                  "f_property_type*d_lake_access",
                  "f_property_type*d_long_term_stays_allowed",
                  "f_property_type*d_luggage_dropoff_allowed",
                  "f_property_type*d_microwave",
                  "f_property_type*d_mosquito_net",
                  "f_property_type*d_outdoor_furniture",
                  "f_property_type*d_pool",
                  "f_property_type*d_private_entrance",
                  "f_property_type*d_private_hot_tub",
                  "f_property_type*d_safe",
                  "f_property_type*d_single_level_home",
                  "f_property_type*d_toaster",
                  "f_property_type*d_trash_compactor",
                  "f_property_type*d_window_ac_unit",
                  "f_property_type*d_have_gril",
                  "f_property_type*d_have_tv",
                  "f_property_type*d_have_dryer",
                  "f_property_type*d_balcony",
                  "f_property_type*d_have_breakfast",
                  "f_property_type*d_have_fitnessgym",
                  "f_property_type*d_family_friendly",
                  "f_property_type*d_have_fireplace",
                  "f_property_type*d_have_clothing_storage",
                  "f_property_type*d_host_identity_verified")

basic_lev  <- c("f_property_type",  "n_accommodates", "n_beds",  "n_days_since_last", "flag_days_since_last")

# Factorized variables
basic_add <- c("f_neighbourhood_cleansed","f_bathroom", "f_bedrooms","f_minimum_nights", "n_availability_365")


reviews <- c("n_reviews_per_month","flag_reviews_per_month")

host <- c("d_host_is_superhost", "d_host_identity_verified")



# Building OLS models

model1 <- " ~ n_accommodates"
model2 <- paste0(" ~ ",paste(basic_lev,collapse = " + "))
model3 <- paste0(" ~ ",paste(c(basic_lev, basic_add,reviews,host, dummies ),collapse = " + "))



```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Do the iteration

library(fixest)
for ( i in 1:4 ){
  print(paste0( "Estimating model: " ,i ))
  # Get the model name
  model_name <-  paste0("model",i)
  model_pretty_name <- paste0("M",i,"")
  # Specify the formula
  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))
  
  # Estimate model on the whole sample
  model_work_data <- feols( formula , data = data_train , vcov='hetero' )
  #  and get the summary statistics
  fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
  BIC <- fs$bic
  r2  <- fs$r2
  rmse_train <- fs$rmse
  ncoeff <- length( model_work_data$coefficients )
  
  # Do the k-fold estimation
  set.seed(seed_val)
  cv_i <- train( formula, data_train, method = "lm",
                 trControl = trainControl(method = "cv", number = k_folds))
  rmse_test <- mean( cv_i$resample$RMSE )
  
  # Save the results
  model_add <- tibble(Model=model_pretty_name, Coefficients=ncoeff,
                      R_squared=r2, BIC = BIC,
                      Training_RMSE = rmse_train, Test_RMSE = rmse_test )
  if ( i == 1 ){
    model_results <- model_add
  } else{
    model_results <- rbind( model_results , model_add )
  }
}



# Check summary table
# Add it to final results

model_results <- rbind( model_results , lasso_add )
model_results

## As per these results, model4 is clearly over fitted as the R-squared comes out to be 1 with a negative BIC.##
## The purpose of model4 was primarily to include all the relevant variables and use it in LASSO to identify predictors with non-zero coefficients.##



```


```{r message=FALSE, warning=FALSE, include=FALSE}
#before random forest

predictors_model3 <- c(basic_lev, basic_add, dummies, host , reviews)
set.seed(95)
system.time({
  ols_model <- train(
    formula(paste0("price ~", paste0(predictors_model3, collapse = " + "))),
    data = data_train,
    method = "lm",
    trControl = train_control
  )
})
ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

ols_model_coeffs_df



# Random Forest

predictors <- c(basic_lev,basic_add,host, reviews, dummies,interactions)

# set tuning
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)
set.seed(95)
system.time({
  rf_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})

rf_model

# auto tuning first - gives 92 predictors

set.seed(95)
system.time({
  rf_model_auto <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    importance = "impurity"
  )
})
rf_model_auto

rf_model_table <- rf_model$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)

##Variable Importance Plots rf_model

rf_model_var_imp <- ranger::importance(rf_model$finalModel)/1000

rf_model_var_imp_df <-
  data.frame(varname = names(rf_model_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_var_imp_df

# to have a quick look
plot(varImp(rf_model))


# have a version with top 10 vars only

var_imp10 <- ggplot(rf_model_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

```

```{r message=FALSE, warning=FALSE, include=FALSE}
##############################
# 2) varimp plot grouped
##############################

# grouped variable importance - keep binaries created off factors together
varnames <- rf_model$finalModel$xNames

f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)

f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)

f_reviews_varnames <- grep("review",varnames, value = TRUE)


#dummies_varnames?
t <-  grep("d_",varnames, value = TRUE)
dummies_varnames <-  t[43:95]


groups <- list(
               Property_type = f_property_type_varnames,
               Reviews = f_reviews_varnames,
               Neighbourhood= f_neighbourhood_cleansed_varnames,
               Amenities = dummies_varnames,bathroom = "f_bathroom",
               last_review = "n_days_since_last",
               n_accommodates = "n_accommodates",
               availability_365="n_availability_365",
               n_beds = "n_beds"
               )

# Need a function to calculate grouped var-imp

group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_var_imp_grouped <- group.importance(rf_model$finalModel, groups)
rf_model_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_var_imp_grouped),
                                          imp = rf_model_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

var_imp_group <- ggplot(rf_model_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()

##Variable Importance Plots rf_model_auto

rf_model_auto_var_imp <- ranger::importance(rf_model_auto$finalModel)/1000
rf_model_auto_var_imp_df <-
  data.frame(varname = names(rf_model_auto_var_imp),imp = rf_model_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Neighbourhood:", varname) ) %>%
  mutate(varname = gsub("f_property_type", "Property type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))
rf_model_auto_var_imp_df

# to have a quick look

plot(varImp(rf_model_auto))

# have a version with top 10 vars only

ggplot(rf_model_auto_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()
data$d_ <- NULL
colnames(data)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
##############################
# 2) varimp plot grouped

##############################
# grouped variable importance - keep binaries created off factors together

varnames_auto <- rf_model_auto$finalModel$xNames

f_neighbourhood_cleansed_varnames_auto <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
f_property_type_varnames_auto <- grep("f_property_type",varnames_auto, value = TRUE)
f_reviews_varnames_auto <- grep("review",varnames_auto, value = TRUE)
dummies_varnames_auto <- t[14:105]


groups_auto <- list(
                    property_type = f_property_type_varnames_auto,
                    reviews = f_reviews_varnames_auto,
                    neighbourhood=f_neighbourhood_cleansed_varnames_auto,
                    Ammenities = dummies_varnames_auto,
                    bathroom = "f_bathroom",
                    last_review = "n_days_sincelast",
                    n_accommodates = "n_accommodates",
                    availability_365="n_availability_365",
                    n_beds = "n_beds" )


# Need a function to calculate grouped var-imp

group.importance <- function(rf.obj, groups_auto) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(ranger::importance(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}

rf_model_auto_var_imp_grouped <- group.importance(rf_model_auto$finalModel, groups)
rf_model_auto_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_auto_var_imp_grouped),
                                               imp = rf_model_auto_var_imp_grouped[,1])  %>%
  mutate(imp_percentage = imp/sum(imp))

ggplot(rf_model_auto_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color='red', size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color='red', size=0.7) +
  ylab("Importance (Percent)") +   xlab("Variable Name") +
  coord_flip() +
  # expand=c(0,0),
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw()



# evaluate random forests

results <- resamples(
  list(
    model_1  = rf_model,
    model_auto  = rf_model_auto
  )
)
summary(results)


#install.packages("rpart")



# CART with built-in pruning

set.seed(95)
system.time({
  cart_model <- train(
    formula(paste0("price ~", paste0(predictors, collapse = " + "))),
    data = data_train,
    method = "rpart",
    tuneLength = 10,
    trControl = train_control
  )
})

cart_model



# Tree graph

rpart.plot(cart_model$finalModel, tweak=1.2, digits=-1, extra=1)


```

```{r message=FALSE, warning=FALSE, include=FALSE}

# GBM

gbm_grid <-  expand.grid(interaction.depth = 5, # complexity of the tree
                         n.trees = 250, # number of iterations, i.e. trees
                         shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
                         n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
)

set.seed(111)
system.time({
  gbm_model <- train(formula(paste0("price ~", paste0(predictors, collapse = " + "))),
                     data = data_train,
                     method = "gbm",
                     trControl = train_control,
                     verbose = FALSE,
                     tuneGrid = gbm_grid)
})
gbm_model
gbm_model$finalModel
# save( gbm_model , file = 'gbm_model.RData' )


# get prediction rmse and add to next summary table
# ---- compare these models

final_models <-
  list("OLS" = ols_model,
       "CART" = cart_model,
       "Random forest 1: Tuning provided" = rf_model,
       "Random forest 2: Auto Tuning" = rf_model_auto,
       "GBM"  = gbm_model)
results <- resamples(final_models) %>% summary()
results

# Model selection is carried out on this CV RMSE
result <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")
result




#########################################################################################
# Partial Dependence Plots for the best model; random forest with specified tuning parameters
#########################################################################################
# 1) Property Type
pdp_f_property_type <- pdp::partial(rf_model_auto, pred.var = "f_property_type",
                                    pred.grid = distinct_(data_test, "f_property_type"),
                                    train = data_train)
pdp_pptype <- pdp_f_property_type %>%
  autoplot( ) +
  geom_point(color='red', size=2) +
  geom_line(color='red', size=1) +
  ylab("Predicted price") +
  xlab("Property Type") +
  theme_bw()

# 2) Number of accommodates
pdp_n_accommodates <- pdp::partial(rf_model_auto, pred.var = "n_accommodates",
                                   pred.grid = distinct_(data_test, "n_accommodates"),
                                   train = data_train)
pdp_acctype <- pdp_n_accommodates %>%
  autoplot( ) +
  geom_point(color='red', size=4) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  #scale_y_continuous(limits=c(80,120), breaks=seq(80,120, by=10)) +
  theme_bw()

# 3) neighborhood

pdp_f_neighbourhood_cleansed <- pdp::partial(rf_model_auto, pred.var = "f_neighbourhood_cleansed",
                                                   pred.grid = distinct_(data_test, "f_neighbourhood_cleansed"),
                                                   train = data_train)


```

```{r message=FALSE, warning=FALSE, include=FALSE}
## Interaction
p1 <- price_diff_by_variables2(data, "f_property_type", "d_have_air_condfan","Property Type", "Have air conditioner")
p2 <- price_diff_by_variables2(data, "f_property_type", "d_family_friendly","Property Type", "Family friendly")
p3 <- price_diff_by_variables2(data, "f_property_type", "d_long_term_stays_allowed","Property Type", "Long term stay allowed")
p4 <- price_diff_by_variables2(data, "f_property_type", "d_carbon_monoxide_alarm","Property Type", "Have carbon monoxide alarm") 

g_interactions <- plot_grid(p1, p2, p3,
                            p4, nrow=2, ncol=2)
```


```{r message=FALSE, warning=FALSE, include=FALSE}
######### create nice summary table of heterogeneity


####
# Subsample performance: RMSE / mean(y) ---------------------------------------
# NOTE  we do this on the holdout set.
#
data_holdout_w_prediction <- data_test %>%
  mutate(predicted_price = predict(rf_model_auto, newdata = data_test))

a <- data_holdout_w_prediction %>%
  mutate(is_low_size = ifelse(n_accommodates <= 3, "small apt", "large apt")) %>%
  group_by(is_low_size) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )



unique(data$n_beds)
c <- data_holdout_w_prediction %>%
  filter(n_beds %in% c("2","3", "4","5","6","7","8","10", "12")) %>%
  group_by(n_beds) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )
d <- data_holdout_w_prediction %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = RMSE(predicted_price, price) / mean(price)
  )

e <- data_holdout_w_prediction %>%
  filter(f_property_type %in% c("apartment", "condo","loft")) %>%
  group_by(f_property_type) %>%
  dplyr::summarise(
    rmse = RMSE(predicted_price, price),
    mean_price = mean(price),
    rmse_norm = rmse / mean_price
  )

colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")

colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
d<- cbind("All", d)
colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")
colnames(e) <- c("", "RMSE", "Mean price", "RMSE/price")
line1 <- c("Apartment size", "", "", "")
line2 <- c("Beds", "", "", "")

line4 <- c("Property Type", "", "", "")
result_3 <- rbind(line1,a,line2, c ,line4, e, d) %>%
  transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
            `RMSE/price` = as.numeric(`RMSE/price`))
result_3

result_3 %>% kbl(caption = "Performances and subsamples", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center", latex_options = "HOLD_position")

```



```{r message=FALSE, warning=FALSE, include=FALSE}
Ylev <- data_test[["price"]]

# Predicted values
prediction_test_pred <- as.data.frame(predict(rf_model_auto, newdata = data_test, interval="predict"))

predictionlev_test <- cbind(data_test[,c("price","n_accommodates")],
                               prediction_test_pred)



# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_test[,3] )
# Check the differences
d$elev <- d$ylev - d$predlev

# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = "purple", size = 1,
             shape = 16, alpha = 1, show.legend=FALSE, na.rm=TRUE) +
  geom_segment(aes(x = 0, y = 0, xend = 80, yend =80), size=0.8, color="black", linetype=2) +
  labs(y = "Price (US dollars)", x = "Predicted price  (US dollars)") +
  theme_bg()
level_vs_pred
```

## Summary
In this we will be discussing the technical aspects of the project in detail. The purpose of this project is to help a small to mid-sized firm price an apartment that can accommodate 2 – 6 people in **Istanbul**. To make this prediction I will be looking at a few key predictors to make the prediction more accurate. These predictors some basic attributes about the property, reviews and ratings, a few variables regarding the host and various kinds of amenities that describe the apartment. For this project I will be using 5 different types of models which include the OLS, Cart, two different Random Forest and GBM. I will be using these models to pick my final prediction based on the prediction power looking at the RMSE and R squared values. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
price_hist <- ggplot(data = df, aes( x = price)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)),fill = "purple", color = "white") +
  theme_bw() +
  scale_y_continuous(labels = label_percent()) +
  ylab("Percent") +
  xlab("Price (USD)")
price_hist
```


## Data Cleaning 
To start working on the models, I had to start with some preliminary cleaning of the data. Before I began cleaning the data my raw data set contained 22,695 observations and 74 variables. I dropped some of the variables in the beginning that included urls and some descriptions that could not be used in the analysis. After doing some rudimentary cleaning I proceeded to sort out the amenities. The amenities required more effort and time in the cleaning process as they were stored in different characters and listing format. For this I first split the amenities in different dummy variables and then they similar amenities were grouped together into one variable for example, the different WIFI and internet variables were all grouped together as one. This helped me in reducing the number of variables, initially, I had 2093 amenities which were reduced to 172 after being grouped together. For more details the data cleaning code has been uploaded here [data_cleaning.R](https://github.com/).

## Data Preparation
The main variable of interest, the price, was also converted to USD as in the raw data the prices were given in Turkish lira. The conversion was 1 USD = 13.56 Turkish Lira. Furthermore, the all the observations with missing price values were dropped. 
The data was further prepared for the analysis by filtering property type ("f_property_type"). I filtered for apartments, condominium and lofts that can host 2 to 6 people as that is the requirement of our company. Some variables were converted to factors like neighborhoods and property type. Some variables like accommodates, beds and bathrooms had to be converted to numerical values as they were not stored in the correct format.
**Imputation**
I also imputed some of the variables that contained missing values. Variables like number of bedrooms, bathrooms and the number of beds had missing values. For bedrooms with missing values, it was assumed that there is at least 1 bedroom like in a studio apartment. For the number of beds ("n_beds"), I assumed that the number of accommodates ("n_accommodates") is highly dependent on this, therefore, I divided the number of accommodates with 1.5 and rounding this to give us the number of beds. After imputation of these predictors, I had a data set which had no missing values. For more details the data preparation code has been uploaded here [data_prep.R](https://github.com/). 

## Variables
Another enormous challenge for this project was the future engineering and deciding which variables to include in my modelling. I did this by dividing my variables into groups of 5:

-	Numeric variables: These variables define size for example, number of beds, number of accommodates, number of bathrooms and bedrooms, the minimum number of nights required to rent the place and the availability of the apartment in 365 days of the year. 
-	Factor variables: These are categorical variables that are either in string or numeric form. For example, the neighborhood the apartment is in or the type of property (apartment, condominium, or a loft).
-	Dummy variables: These are binary variables mostly describing the amenities of a property like the WIFI, pool, beachfront facing property, kitchen supplies, elevator etc.
-	Review variables: These describe the reviews and ratings characteristics for an individual property. Number of reviews, the rating for those reviews and the mean monthly reviews received. 
-	Host variables: These variables describe the characteristics of the host of the property. They are binary variables like host is a superhost or if the host is verified. 

## Exploratory Data Analysis
Once all the data was cleaned and the variables were grouped, we looked at the main target variable which is the price. The price distribution was close to normal, hence, there was no need of taking a log of the variable. 
The next step was to look for interactions, for this I checked the relationship between property type and some of the amenities and if there was a noticeable price difference then the dummy variable was included as an interaction term with the main indepedent variable of focus which which was property type.

```{r echo=FALSE, message=FALSE, warning=FALSE}
g_interactions
```


## Cross validation and holdout 
Once all the data was prepped for doing our regression analysis, I divided the data into the test and train samples, where 80% of the observations were part of the train sample and 20% of the observations were part of the test sample. These test and train samples were used in all machine learning models.

## LASSO - Variable Selection
I first used LASSO to check variable relation to price and based on that I selected all the non-zero coefficients and included them in my OLS models. The variables with coefficients as 0 were not included as part of the analysis as it showed through the LASSO select that they had no relation to the price of the property.

## Linear Regression
I first started with building my OLS model with the LASSO approved variables. The first model was only with n_accommodates variable, model 2 and 3 had 7 and 91 variables respectively and model 4 was the overfitted model with all the variables. According to my OLS regression results, model 3 proved to be the best predictive model out of these 4. As it can be seen below

Add table for OLS results (model_results)
```{r echo=FALSE, message=FALSE, warning=FALSE}
model_results %>% kbl(caption = "OLS Model Results", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center", latex_options = "HOLD_position")
```


Even though the BIC value increases after the model 1, the increase in the value is not that significant from model 1 to model 3 and model 3 has the lowest RMSE in the training set. Furthermore, looking at the R squared value of the model we can say that this model explains 34.7% of the variation in price. The difference is however a minor. When BIC and Cross validation produce conflicting results, we should prefer to side with the cross-validation results since it’s not based on auxiliary assumptions (BEKES, 2021). Since the difference between them is small it will not be big mistake if I pick model 3 over model 1 and 2. 

## Random Forest

The Random Forest is a machine learning algorithm that uses regression trees. The software creates hundreds of regression trees and combines them in one and gives an average. Since it uses bootstrap aggregation, the model is able to produce accurate results. For this regression model I will be using the same training and holdout samples that were used for OLS and all other models alongside the tuning parameters with cross validation so that I am able to find the RMSE with lowest possible value.

For the number for variables, I picked the range from 8 to 12 and for the terminal nodes I chose 5-10-15. Based on my regression results the lowest RMSE value was at 5 for the terminal nodes and 12  variables in each node giving me an RMSE value of 12.4 as it can be seen from the table below. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
rf_model_table %>% kbl(caption = "Random forest RMSE by tuning parameters", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center", latex_options = "HOLD_position")

```



I also used the autotuned version of the model, in which the algorithm picks the variable and node values automatically. The cross-validated RMSE for autotune produced a cross-validated RMSE of 12.3 with 5 terminal nodes and 92 variables for each split. 

## Model Selection

After running all my models, we had to select one model in the end and we made this selection by using a loss function for which mean squared error (MSE or RMSE) was used.  was I decided that the lowest the RSME value was for Random Forest – Autotuning version which was 12.3.


```{r echo=FALSE, message=FALSE, warning=FALSE}
result %>% kbl(caption = "Prdictive performance of different models", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center")
```

## Variable Importance

The variable importance helps us in identifying the important variables that impact price. In my analysis the room availability, number of accommodates, days since last review, reviews per month and if the property has an air conditioner or fan were the top 5. The top 10 variables according to importance can be seen below. Furthermore, I grouped the variables in this the graph shows the sum of gains (in terms of MSE reduction). In order to get a bird eye view as seen in the graph below, amenities account for 50% of the variable importance. 

## Partial Dependencies

To further investigate I created partial dependency plot for the number of accommodates and the predicted price and as we can see in the graph below there is a fairly linear relationship between the number of people accommodated and price. We also created a plot with property type and price and as in the graph below it can be seen that renting a condo is more expensive than an apartment.

## Subsample

To further analyze and check the robustness of the model I divided the data into smaller samples to check the RMSE values of certain groups as it can be seen in the table below. According to apartment size there was a little difference in the RSME/price value and hence we can say that the model was fairly balanced. As for the number of beds the RSME/price value was also similar, however, beds 10 and 12 had a different value hence it will be harder to predict prices for those. I also compared property type and the results suggest that apartments and condos have similar results, however, it’ll be difficult to predict the prices of lofts due higher RMSE value. Therefore, in conclusion the predicted errors are similar across apartment size and the number of bed, but loft prices are harder to predict. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
result_3 %>% kbl(caption = "Performances and subsamples", escape = FALSE) %>%
  kable_classic(full_width = F, html_font = "Cambria") %>%
  kable_styling( position = "center", latex_options = "HOLD_position")
```


## Conclusion

After conducting this analysis, according to the results my best model was the Random Forest model with autotuning as that gave the lowest RMSE value. As it can be seen in the graph below, there are some points for which our model predicts the price exactly to what the actual price is. But the model also overfits and underfits as there are points above and below the 45 degree line.
To draw a comparison with the case study in Gabor’s book, both the regressions were fairly similar, however my RMSE value was lower as it was around 12 and the RMSE value in the book was around 40 this could be due to many reasons. Firstly, my city was Istanbul and prices in Istanbul and lower compared to prices in London and in recent time the Turkish Lira has depreciated a lot causing the prices in USD to fall as well. 

```{r echo=FALSE, message=FALSE, warning=FALSE}
level_vs_pred
```



